{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d360b1ea",
   "metadata": {},
   "source": [
    "# Regression Algorithms: Theory, Math, and Implementation\n",
    "\n",
    "Regression is a supervised learning technique used to predict continuous numerical values based on input features. In this notebook, we will explore five fundamental regression algorithms, understand their mathematical foundations, and visualize their decision boundaries using Python and `scikit-learn`.\n",
    "\n",
    "---\n",
    "### Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19e8ae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from ipywidgets import interact, IntSlider, FloatSlider\n",
    "\n",
    "# Global plot settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Generate a shared complex dataset for SVR, Tree, and Forest\n",
    "np.random.seed(42)\n",
    "X_complex = np.sort(5 * np.random.rand(100, 1), axis=0)\n",
    "y_complex = np.sin(X_complex).ravel()\n",
    "y_complex[::5] += 3 * (0.5 - np.random.rand(20)) # Add noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540afa0c",
   "metadata": {},
   "source": [
    "### 1. Linear Regression\n",
    "Linear Regression assumes a linear relationship between the input variables ($X$) and the single output variable ($y$). It tries to find the \"line of best fit\" that minimizes the error between the predicted and actual values.\n",
    "\n",
    "The hypothesis function for a simple linear regression is:\n",
    "$$y = \\beta_0 + \\beta_1 x + \\epsilon$$\n",
    "\n",
    "Where:\n",
    "* $y$ is the predicted output.\n",
    "* $\\beta_0$ is the y-intercept.\n",
    "* $\\beta_1$ is the slope (coefficient).\n",
    "* $\\epsilon$ is the error term.\n",
    "\n",
    "The model learns by minimizing the **Mean Squared Error (MSE)** Cost Function ($J$):\n",
    "$$J(\\beta_0, \\beta_1) = \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_i))^2$$\n",
    "\n",
    "While it doesn't have complex hyperparameters like depth or penalty, we can visualize how it reacts to different levels of noise in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a977ae85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b6e1a4f4714417b7281cff46c1d2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='noise', max=5.0, step=0.5), Output()), _dom_classes=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(noise=FloatSlider(min=0.0, max=5.0, step=0.5, value=1.0))\n",
    "def plot_linear_regression(noise):\n",
    "    np.random.seed(42)\n",
    "    X = 2 * np.random.rand(100, 1)\n",
    "    y = 4 + 3 * X + np.random.randn(100, 1) * noise\n",
    "    \n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(X, y)\n",
    "    \n",
    "    X_new = np.array([[0], [2]])\n",
    "    y_predict = lin_reg.predict(X_new)\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(X, y, color=\"blue\", alpha=0.6, label=\"Training data\")\n",
    "    plt.plot(X_new, y_predict, \"r-\", linewidth=2, label=f\"Fit: y = {lin_reg.intercept_[0]:.2f} + {lin_reg.coef_[0][0]:.2f}x\")\n",
    "    plt.title(f\"Linear Regression (Noise level: {noise})\")\n",
    "    plt.ylim(0, 15)\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea793f3",
   "metadata": {},
   "source": [
    "### 2. Polynomial Regression\n",
    "When the data shows a non-linear relationship, fitting a straight line will result in underfitting. Polynomial Regression solves this by adding powers of the original features as new features, allowing the model to fit a curve.\n",
    "\n",
    "We extend the linear equation to a polynomial of degree $d$:\n",
    "$$y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\dots + \\beta_d x^d + \\epsilon$$\n",
    "\n",
    "Under the hood, this is still a *linear* model because the coefficients ($\\beta$) are linear, even though the features ($x^2, x^3$) are not.\n",
    "\n",
    "Use the slider to increase the `degree`. Notice how a low degree *underfits* the curve, while a very high degree *overfits* by trying to connect every single noisy point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfda1006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd56ba1cc3cd41c78ecb0d3162de56fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=2, description='degree', max=15, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(degree=IntSlider(min=1, max=15, step=1, value=2))\n",
    "def plot_polynomial_regression(degree):\n",
    "    np.random.seed(42)\n",
    "    m = 100\n",
    "    X = 6 * np.random.rand(m, 1) - 3\n",
    "    y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
    "    \n",
    "    poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly_features.fit_transform(X)\n",
    "    \n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(X_poly, y)\n",
    "    \n",
    "    X_new = np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "    y_new = lin_reg.predict(poly_features.transform(X_new))\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(X, y, color=\"blue\", alpha=0.6, label=\"Data\")\n",
    "    plt.plot(X_new, y_new, \"r-\", linewidth=2, label=f\"Degree {degree} fit\")\n",
    "    plt.title(\"Polynomial Regression\")\n",
    "    plt.ylim(-2, 10)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d88a7f3",
   "metadata": {},
   "source": [
    "### 3. Support Vector Regression (SVR)\n",
    "Unlike Linear Regression which tries to minimize the error, SVR tries to fit the error *within a certain threshold* (called the $\\epsilon$-tube). It only cares about the points that fall outside this tube (the support vectors).\n",
    "\n",
    "The goal is to find a function $f(x) = \\langle w, x \\rangle + b$ that is as \"flat\" as possible (minimizing the norm of $w$) while keeping the prediction errors within a margin $\\epsilon$:\n",
    "\n",
    "$$\\text{Minimize: } \\frac{1}{2} ||w||^2$$\n",
    "$$\\text{Subject to: } |y_i - (\\langle w, x_i \\rangle + b)| \\le \\epsilon$$\n",
    "\n",
    "By using the \"Kernel Trick\" (like the Radial Basis Function or RBF), SVR can efficiently map data into higher dimensions to solve highly non-linear problems.\n",
    "\n",
    "* **C:** The penalty parameter. A high `C` strictly forces the model to fit the data (potentially overfitting), while a low `C` allows a smoother curve.\n",
    "* **Epsilon ($\\epsilon$):** The width of the tube where no penalty is given to errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f5d3e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a4c1e137d54bb1ba64a5803c268427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=100.0, description='C', max=1000.0, min=0.1, step=10.0), FloatSlider(v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(C=FloatSlider(min=0.1, max=1000, step=10, value=100),\n",
    "          epsilon=FloatSlider(min=0.01, max=1.0, step=0.05, value=0.1))\n",
    "def plot_svr(C, epsilon):\n",
    "    svr_rbf = SVR(kernel='rbf', C=C, gamma=0.1, epsilon=epsilon)\n",
    "    svr_rbf.fit(X_complex, y_complex)\n",
    "    \n",
    "    X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "    y_pred = svr_rbf.predict(X_test)\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(X_complex, y_complex, color=\"darkorange\", label=\"data\")\n",
    "    plt.plot(X_test, y_pred, color=\"navy\", lw=2, label=\"SVR RBF\")\n",
    "    plt.title(f\"Support Vector Regression (C={C}, epsilon={epsilon:.2f})\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038edbd1",
   "metadata": {},
   "source": [
    "### 4. Decision Tree Regression\n",
    "A Decision Tree breaks down a dataset into smaller and smaller subsets by asking a series of True/False questions. The final predictions are made at the \"leaves\" of the tree. It is highly interpretable but prone to overfitting.\n",
    "\n",
    "The algorithm splits the data into two regions ($R_1$ and $R_2$) at a specific feature $j$ and threshold $s$. It searches for the split that minimizes the Mean Squared Error of the resulting regions:\n",
    "\n",
    "$$J(j, s) = \\min_{j, s} \\left[ \\min_{c_1} \\sum_{x_i \\in R_1(j,s)} (y_i - c_1)^2 + \\min_{c_2} \\sum_{x_i \\in R_2(j,s)} (y_i - c_2)^2 \\right]$$\n",
    "\n",
    "Where $c_1$ and $c_2$ are the mean target values in regions $R_1$ and $R_2$.\n",
    "\n",
    "Use the slider to increase `max_depth`. A depth of 1 or 2 is too simple, but if you push it to 10, watch how the model creates rigid, stepped jumps to perfectly hit the noise (classic overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18d24dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20a0dd5756b480f8bb7a5536a7e0a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=3, description='max_depth', max=15, min=1), Output()), _dom_classes=('wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(max_depth=IntSlider(min=1, max=15, step=1, value=3))\n",
    "def plot_decision_tree(max_depth):\n",
    "    tree_reg = DecisionTreeRegressor(max_depth=max_depth)\n",
    "    tree_reg.fit(X_complex, y_complex)\n",
    "    \n",
    "    X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "    y_pred = tree_reg.predict(X_test)\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(X_complex, y_complex, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"data\")\n",
    "    plt.plot(X_test, y_pred, color=\"cornflowerblue\", label=f\"Tree depth={max_depth}\", linewidth=2)\n",
    "    plt.title(\"Decision Tree Regression\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ac77ee",
   "metadata": {},
   "source": [
    "### 5. Random Forest Regression\n",
    "Random Forest is an ensemble method. Instead of relying on one Decision Tree, it builds multiple trees on random subsets of the data (Bagging) and averages their predictions. This significantly reduces the overfitting seen in single Decision Trees.\n",
    "\n",
    "If a Random Forest contains $B$ decision trees, the final prediction $\\hat{y}$ for an input $x$ is simply the average of the predictions from all individual trees ($f_b(x)$):\n",
    "\n",
    "$$\\hat{y} = \\frac{1}{B} \\sum_{b=1}^{B} f_b(x)$$\n",
    "\n",
    "* **n_estimators:** The number of trees in the forest. Notice how adding more trees smooths out the harsh steps of a single decision tree.\n",
    "* **max_depth:** How deep each individual tree can go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08c54aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b3c1370c934c998d372a809cdab23e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='n_estimators', max=200, min=1, step=10), IntSlider(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(n_estimators=IntSlider(min=1, max=200, step=10, value=10),\n",
    "          max_depth=IntSlider(min=1, max=15, step=1, value=4))\n",
    "def plot_random_forest(n_estimators, max_depth):\n",
    "    rf_reg = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
    "    rf_reg.fit(X_complex, y_complex)\n",
    "    \n",
    "    X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "    y_pred = rf_reg.predict(X_test)\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(X_complex, y_complex, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"data\")\n",
    "    plt.plot(X_test, y_pred, color=\"green\", label=f\"RF (Trees={n_estimators}, Depth={max_depth})\", linewidth=2)\n",
    "    plt.title(\"Random Forest Regression\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
