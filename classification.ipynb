{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb2b4e9d",
   "metadata": {},
   "source": [
    "# Classification Algorithms: Theory, Math, and Implementation\n",
    "\n",
    "Classification is a supervised learning technique used to predict discrete class labels based on input features. In this notebook, we will explore five fundamental classification algorithms, understand their mathematical foundations, and visualize their decision boundaries using Python and `scikit-learn`.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d751079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from ipywidgets import interact, IntSlider, FloatSlider, FloatLogSlider\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "np.random.seed(42)\n",
    "X_class, y_class = make_classification(n_samples=300, n_features=2, n_informative=2, \n",
    "                                       n_redundant=0, n_clusters_per_class=1, \n",
    "                                       flip_y=0.1, class_sep=1.2, random_state=42)\n",
    "\n",
    "def plot_decision_boundary(model, X, y, title):\n",
    "    \"\"\"Helper function to plot interactive decision boundaries.\"\"\"\n",
    "    h = .02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', cmap='coolwarm', s=40)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7fd60f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 1. Logistic Regression\n",
    "Despite its name, Logistic Regression is a **classification** algorithm. It uses a logistic (sigmoid) function to map predictions to probabilities (between 0 and 1) for binary classification.\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "It transforms the output of a linear equation using the Sigmoid function $\\sigma(z)$:\n",
    "\n",
    "$$z = \\left(\\sum_{i=1}^{n} w_i x_i\\right) + b$$\n",
    "\n",
    "In vector notation, where $w$ is the weight vector and $X$ is the feature vector, this is written as a dot product:\n",
    "$$z = w \\cdot X + b$$\n",
    "\n",
    "Using sigmoid function where the input will be $z$ and the probability will be between 0 and 1.\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "Because **Logistic Regression** use the **Log-Odds** method therefore\n",
    "\n",
    "$$ \\frac{P(x)}{1-P(x)} = e^{z}$$\n",
    "\n",
    "then\n",
    "\n",
    "$$\\frac{P}{1-P} = e^{w \\cdot X + b}$$\n",
    "\n",
    "$$P = \\frac{e^{w \\cdot X + b}}{1 + e^{w \\cdot X + b}}$$\n",
    "\n",
    "Using the rule of negative exponents ($\\frac{1}{e^z} = e^{-z}$), this simplifies to the final formula:\n",
    "\n",
    "$$\\sigma(X; w, b) = P(y=1|X) = \\frac{1}{1 + e^{-(w \\cdot X + b)}}$$\n",
    "\n",
    "Where:\n",
    "* $P(y=1|X)$ is the probability that the data point belongs to Class 1.\n",
    "* $w$ represents the learned weights and $b$ is the bias (intercept).\n",
    "* $e$ is the base of the natural logarithm.\n",
    "\n",
    "or\n",
    "\n",
    "$$P(y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n)}}$$\n",
    "\n",
    "Where:\n",
    "* $P(y=1|X)$ is the probability that the data point belongs to class 1.\n",
    "* $\\beta_0, \\beta_1, \\dots$ are the learned weights.\n",
    "\n",
    "To train the Logistic Regression model, we need to measure how well our current weights ($w$) and bias ($b$) are performing. We do this using a Cost Function. For Logistic Regression, we use **Log Loss** (also known as Binary Cross-Entropy) instead of Mean Squared Error, because MSE combined with the Sigmoid function creates a non-convex function with many local minima.\n",
    "\n",
    "For a single training example, the cost is:\n",
    "$$Cost(\\hat{y}, y) = -[y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})]$$\n",
    "\n",
    "For the entire dataset of $m$ samples, the average cost function $J(w, b)$ is:\n",
    "$$J(w, b) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right]$$\n",
    "\n",
    "Where:\n",
    "* $m$ is the total number of training examples.\n",
    "* $y^{(i)}$ is the actual class label (0 or 1) for the $i$-th example.\n",
    "* $\\hat{y}^{(i)}$ is the predicted probability $\\sigma(w \\cdot x^{(i)} + b)$ for the $i$-th example.\n",
    "\n",
    "To find the gradient, we take the partial derivatives of the Cost Function with respect to the weights ($w_j$) and the bias ($b$). Thanks to the properties of the sigmoid derivative, the calculus simplifies beautifully to:\n",
    "\n",
    "**Derivative with respect to weights ($w_j$):**\n",
    "$$\\frac{\\partial J}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( \\hat{y}^{(i)} - y^{(i)} \\right) x_j^{(i)}$$\n",
    "\n",
    "**Derivative with respect to bias ($b$):**\n",
    "$$\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( \\hat{y}^{(i)} - y^{(i)} \\right)$$\n",
    "\n",
    "During training, the algorithm repeatedly updates the weights and bias using these gradients, multiplied by a learning rate ($\\alpha$). The learning rate dictates the size of the steps taken towards the minimum.\n",
    "\n",
    "$$w_j := w_j - \\alpha \\frac{\\partial J}{\\partial w_j}$$\n",
    "$$b := b - \\alpha \\frac{\\partial J}{\\partial b}$$\n",
    "\n",
    "This process repeats iteratively until the cost function converges to its minimum, at which point the optimal weights and bias have been found!\n",
    "\n",
    "\n",
    "**Example Problem:**\n",
    "* **Medicine:** Predicting if a tumor is Malignant (1) or Benign (0) based on continuous features like size and texture.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a58f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a29668a54a42ff8d21ec8fc17edc83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatLogSlider(value=1.0, description='C (1/Penalty)', max=3.0, min=-3.0, step=0.5), Out…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(C=FloatLogSlider(value=1.0, min=-3, max=3, step=0.5, description='C (1/Penalty)'))\n",
    "def plot_dynamic_sigmoid(C):\n",
    "    \n",
    "    log_reg = LogisticRegression(C=C, random_state=42)\n",
    "    log_reg.fit(X_class, y_class)\n",
    "\n",
    "    z = np.dot(X_class, log_reg.coef_[0]) + log_reg.intercept_[0]\n",
    "\n",
    "    def sigmoid(z_val):\n",
    "        return 1 / (1 + np.exp(-z_val))\n",
    "\n",
    "    z_smooth = np.linspace(z.min() - 1, z.max() + 1, 300)\n",
    "    probabilities_smooth = sigmoid(z_smooth)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    plt.scatter(z[y_class==0], y_class[y_class==0], color='blue', alpha=0.5, label='Class 0 (Actual)')\n",
    "    plt.scatter(z[y_class==1], y_class[y_class==1], color='red', alpha=0.5, label='Class 1 (Actual)')\n",
    "\n",
    "    plt.plot(z_smooth, probabilities_smooth, color='green', linewidth=3, label='Sigmoid Curve $P(y=1)$')\n",
    "\n",
    "    plt.axhline(y=0.5, color='black', linestyle='--', alpha=0.7, label='Decision Threshold (0.5)')\n",
    "    plt.axvline(x=0.0, color='gray', linestyle=':', alpha=0.7)\n",
    "\n",
    "    plt.title(f\"Dynamic Logistic Regression Sigmoid Curve (C={C:.2e})\", fontsize=15)\n",
    "    plt.xlabel(\"Linear Combination: $z = w_1 x_1 + w_2 x_2 + b$\", fontsize=12)\n",
    "    plt.ylabel(\"Probability $P(y=1)$\", fontsize=12)\n",
    "    plt.legend(loc='center right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5e6f50",
   "metadata": {},
   "source": [
    "* **Slider (`C`):** This controls the inverse of regularization strength. \n",
    "    * When `C` is very small (strong regularization), the weights ($w_1, w_2$) are penalized and kept small. Notice how the data points cluster tightly around $z=0$, meaning the model is less confident and predicts probabilities closer to 0.5.\n",
    "    * When `C` is large (weak regularization), the weights can grow larger. The data points spread out further along the $z$-axis, moving towards the flat tails of the sigmoid curve. This shows the model making more extreme, highly confident predictions (closer to 0.0 or 1.0).\n",
    "* **The Curve:** The green sigmoid curve itself retains the exact same mathematical shape ($P = 1 / (1 + e^{-z})$). The dynamic changes happen in how the *data points* are projected onto this curve based on the learned weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5eb106c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22dcc4fc4f694004b0f3b3669bccab45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatLogSlider(value=1.0, description='C (1/Penalty)', max=3.0, min=-3.0, step=0.5), Out…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(C=FloatLogSlider(value=1.0, min=-3, max=3, step=0.5, description='C (1/Penalty)'))\n",
    "def plot_logistic_regression(C):\n",
    "    log_reg = LogisticRegression(C=C, random_state=42)\n",
    "    log_reg.fit(X_class, y_class)\n",
    "    plot_decision_boundary(log_reg, X_class, y_class, f\"Logistic Regression (C={C:.2e})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9939c83",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 2. K-Nearest Neighbors (K-NN)\n",
    "K-NN is an intuitive, instance-based learning algorithm. It classifies a new data point based on the majority class of its 'k' nearest neighbors in the feature space.\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "To find the \"nearest\" neighbors, it relies on distance metrics. The most common is the Euclidean Distance between two points $p$ and $q$:\n",
    "$$d(p, q) = \\sqrt{\\sum_{i=1}^{n}(q_i - p_i)^2}$$\n",
    "\n",
    "**Example Problem:**\n",
    "* **Pattern Recognition:** Recognizing handwritten digits (0-9) based on the pixel similarity to known, labeled digit images.\n",
    "\n",
    "Use the slider to change `k` (the number of neighbors). Notice how a very low `k` creates highly fragmented boundaries (overfitting to noise), while a high `k` creates very smooth, generalized boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95ab676a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2630eeedba224e0582bdf3ad3bfc5a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=5, description='k neighbors', max=50, min=1), Output()), _dom_classes=('…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(n_neighbors=IntSlider(min=1, max=50, step=1, value=5, description='k neighbors'))\n",
    "def plot_knn(n_neighbors):\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    knn.fit(X_class, y_class)\n",
    "    plot_decision_boundary(knn, X_class, y_class, f\"K-Nearest Neighbors (k={n_neighbors})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4994d2",
   "metadata": {},
   "source": [
    "### 3. Support Vector Machines (SVM)\n",
    "\n",
    "SVM is a powerful classifier that finds the optimal hyperplane that best separates different classes while maximizing the margin (distance) between the classes' closest points (the support vectors).\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "For linearly separable data, the goal is to maximize the margin $\\frac{2}{||w||}$. This is framed as a minimization problem:\n",
    "$$\\text{Minimize: } \\frac{1}{2} ||w||^2$$\n",
    "$$\\text{Subject to: } y_i(\\langle w, x_i \\rangle + b) \\ge 1 \\text{ for all } i$$\n",
    "\n",
    "For non-linear data, SVM uses the **Kernel Trick** (like Radial Basis Function - RBF) to map data into higher dimensions where a hyperplane can separate it.\n",
    "\n",
    "**Example Problem:**\n",
    "* **Neuroscience:** Classifying EEG signals to detect if a subject is in a \"Resting\" vs. \"Active\" state.\n",
    "\n",
    "**Hyperparameters:**\n",
    "* **C (Penalty):** High C creates a strict boundary with fewer margin violations. Low C encourages a wider, softer margin.\n",
    "* **Gamma:** Defines how far the influence of a single training example reaches. High gamma creates tight, complex boundaries around individual points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78864bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e3009d3b878498e84a41ee42fd7fe21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatLogSlider(value=1.0, description='C (Penalty)', max=3.0, min=-2.0, step=0.5), Float…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(C=FloatLogSlider(value=1.0, min=-2, max=3, step=0.5, description='C (Penalty)'),\n",
    "          gamma=FloatLogSlider(value=0.1, min=-3, max=1, step=0.5, description='Gamma'))\n",
    "def plot_svm(C, gamma):\n",
    "    svm_clf = SVC(kernel='rbf', C=C, gamma=gamma, random_state=42)\n",
    "    svm_clf.fit(X_class, y_class)\n",
    "    plot_decision_boundary(svm_clf, X_class, y_class, f\"SVM RBF (C={C:.2f}, Gamma={gamma:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d120105c",
   "metadata": {},
   "source": [
    "### 4. Naive Bayes\n",
    "Naive Bayes is a fast, probabilistic classifier based on Bayes' Theorem. It relies on the \"naive\" assumption of conditional independence between every pair of features given the class label.\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "Based on Bayes' theorem, the probability of class $y$ given a set of features $X = (x_1, \\dots, x_n)$ is:\n",
    "$$P(y \\mid x_1, \\dots, x_n) = \\frac{P(x_1, \\dots, x_n \\mid y) P(y)}{P(x_1, \\dots, x_n)}$$\n",
    "\n",
    "Assuming independent features, this simplifies to maximizing the numerator:\n",
    "$$\\hat{y} = \\arg\\max_{y} P(y) \\prod_{i=1}^{n} P(x_i \\mid y)$$\n",
    "\n",
    "**Example Problem:**\n",
    "* **Natural Language Processing (NLP):** Classifying emails as \"Spam\" or \"Inbox\" based on the frequency of certain words (e.g., \"Free\", \"Winner\", \"Meeting\").\n",
    "\n",
    "The slider below controls `var_smoothing`, which artificially adds a tiny portion of the largest variance to all features to prevent division-by-zero errors and mathematically stabilize the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be7bf0e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4296d17004384ec9ac97769e96f4962a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatLogSlider(value=1e-09, description='Smoothing', max=-1.0, min=-10.0, step=1.0), Out…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(var_smoothing=FloatLogSlider(value=1e-9, min=-10, max=-1, step=1, description='Smoothing'))\n",
    "def plot_naive_bayes(var_smoothing):\n",
    "    nb_clf = GaussianNB(var_smoothing=var_smoothing)\n",
    "    nb_clf.fit(X_class, y_class)\n",
    "    plot_decision_boundary(nb_clf, X_class, y_class, f\"Gaussian Naive Bayes (Smoothing={var_smoothing:.1e})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787c1580",
   "metadata": {},
   "source": [
    "### 5. Decision Tree Classification\n",
    "\n",
    "A Decision Tree classifies data by recursively splitting the dataset based on feature values. It creates a flowchart-like tree structure by asking a sequence of true/false questions.\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "The tree splits nodes by finding the feature and threshold that minimize **Impurity**. A common metric is the **Gini Impurity**:\n",
    "$$Gini(Q) = 1 - \\sum_{k=1}^{C} p_{k}^2$$\n",
    "Where $p_k$ is the ratio of class $k$ instances among the training instances in node $Q$. The tree tries to make each resulting leaf node as homogeneous (pure) as possible.\n",
    "\n",
    "**Example Problem:**\n",
    "* **Medical Diagnosis:** Diagnosing a disease based on a checklist of symptoms (e.g., Does patient have fever? Yes $\\rightarrow$ Cough? Yes $\\rightarrow$ Flu).\n",
    "\n",
    "The slider below controls `max_depth`. A shallow tree may underfit the data, but an unrestricted deep tree will memorize the training data (overfitting) by creating microscopic decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "899c92d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd86e6eae42439da469544421c5a457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=3, description='Max Depth', max=15, min=1), Output()), _dom_classes=('wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(max_depth=IntSlider(min=1, max=15, step=1, value=3, description='Max Depth'))\n",
    "def plot_decision_tree(max_depth):\n",
    "    tree_clf = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
    "    tree_clf.fit(X_class, y_class)\n",
    "    plot_decision_boundary(tree_clf, X_class, y_class, f\"Decision Tree (Max Depth={max_depth})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
