{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb2b4e9d",
   "metadata": {},
   "source": [
    "# Classification Algorithms: Theory, Math, and Implementation\n",
    "\n",
    "Classification is a supervised learning technique used to predict discrete class labels based on input features. In this notebook, we will explore five fundamental classification algorithms, understand their mathematical foundations, and visualize their decision boundaries using Python and `scikit-learn`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d751079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from ipywidgets import interact, IntSlider, FloatSlider, FloatLogSlider\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "np.random.seed(42)\n",
    "X_class, y_class = make_classification(n_samples=300, n_features=2, n_informative=2, \n",
    "                                       n_redundant=0, n_clusters_per_class=1, \n",
    "                                       flip_y=0.1, class_sep=1.2, random_state=42)\n",
    "\n",
    "def plot_decision_boundary(model, X, y, title):\n",
    "    \"\"\"Helper function to plot interactive decision boundaries.\"\"\"\n",
    "    h = .02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', cmap='coolwarm', s=40)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7fd60f",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression\n",
    "Despite its name, Logistic Regression is a **classification** algorithm. It uses a logistic (sigmoid) function to map predictions to probabilities (between 0 and 1) for binary classification.\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "It transforms the output of a linear equation using the Sigmoid function $\\sigma(z)$:\n",
    "$$P(y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n)}}$$\n",
    "\n",
    "Where:\n",
    "* $P(y=1|X)$ is the probability that the data point belongs to class 1.\n",
    "* $\\beta_0, \\beta_1, \\dots$ are the learned weights.\n",
    "\n",
    "**Example Problem:**\n",
    "* **Medicine:** Predicting if a tumor is Malignant (1) or Benign (0) based on continuous features like size and texture.\n",
    "\n",
    "The slider below controls `C`, the inverse of regularization strength. Smaller values specify stronger regularization, forcing a simpler decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eb106c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6246a8c6d8647529224ed676bfc7e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatLogSlider(value=1.0, description='C (1/Penalty)', max=3.0, min=-3.0, step=0.5), Out…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(C=FloatLogSlider(value=1.0, min=-3, max=3, step=0.5, description='C (1/Penalty)'))\n",
    "def plot_logistic_regression(C):\n",
    "    log_reg = LogisticRegression(C=C, random_state=42)\n",
    "    log_reg.fit(X_class, y_class)\n",
    "    plot_decision_boundary(log_reg, X_class, y_class, f\"Logistic Regression (C={C:.2e})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9939c83",
   "metadata": {},
   "source": [
    "### 2. K-Nearest Neighbors (K-NN)\n",
    "K-NN is an intuitive, instance-based learning algorithm. It classifies a new data point based on the majority class of its 'k' nearest neighbors in the feature space.\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "To find the \"nearest\" neighbors, it relies on distance metrics. The most common is the Euclidean Distance between two points $p$ and $q$:\n",
    "$$d(p, q) = \\sqrt{\\sum_{i=1}^{n}(q_i - p_i)^2}$$\n",
    "\n",
    "**Example Problem:**\n",
    "* **Pattern Recognition:** Recognizing handwritten digits (0-9) based on the pixel similarity to known, labeled digit images.\n",
    "\n",
    "Use the slider to change `k` (the number of neighbors). Notice how a very low `k` creates highly fragmented boundaries (overfitting to noise), while a high `k` creates very smooth, generalized boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95ab676a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfae12613132475880395c20075150cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=5, description='k neighbors', max=50, min=1), Output()), _dom_classes=('…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(n_neighbors=IntSlider(min=1, max=50, step=1, value=5, description='k neighbors'))\n",
    "def plot_knn(n_neighbors):\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    knn.fit(X_class, y_class)\n",
    "    plot_decision_boundary(knn, X_class, y_class, f\"K-Nearest Neighbors (k={n_neighbors})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4994d2",
   "metadata": {},
   "source": [
    "### 3. Support Vector Machines (SVM)\n",
    "\n",
    "SVM is a powerful classifier that finds the optimal hyperplane that best separates different classes while maximizing the margin (distance) between the classes' closest points (the support vectors).\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "For linearly separable data, the goal is to maximize the margin $\\frac{2}{||w||}$. This is framed as a minimization problem:\n",
    "$$\\text{Minimize: } \\frac{1}{2} ||w||^2$$\n",
    "$$\\text{Subject to: } y_i(\\langle w, x_i \\rangle + b) \\ge 1 \\text{ for all } i$$\n",
    "\n",
    "For non-linear data, SVM uses the **Kernel Trick** (like Radial Basis Function - RBF) to map data into higher dimensions where a hyperplane can separate it.\n",
    "\n",
    "**Example Problem:**\n",
    "* **Neuroscience:** Classifying EEG signals to detect if a subject is in a \"Resting\" vs. \"Active\" state.\n",
    "\n",
    "**Hyperparameters:**\n",
    "* **C (Penalty):** High C creates a strict boundary with fewer margin violations. Low C encourages a wider, softer margin.\n",
    "* **Gamma:** Defines how far the influence of a single training example reaches. High gamma creates tight, complex boundaries around individual points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78864bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b733a715fc433698d92ec9ad1ede64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatLogSlider(value=1.0, description='C (Penalty)', max=3.0, min=-2.0, step=0.5), Float…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(C=FloatLogSlider(value=1.0, min=-2, max=3, step=0.5, description='C (Penalty)'),\n",
    "          gamma=FloatLogSlider(value=0.1, min=-3, max=1, step=0.5, description='Gamma'))\n",
    "def plot_svm(C, gamma):\n",
    "    svm_clf = SVC(kernel='rbf', C=C, gamma=gamma, random_state=42)\n",
    "    svm_clf.fit(X_class, y_class)\n",
    "    plot_decision_boundary(svm_clf, X_class, y_class, f\"SVM RBF (C={C:.2f}, Gamma={gamma:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d120105c",
   "metadata": {},
   "source": [
    "### 4. Naive Bayes\n",
    "Naive Bayes is a fast, probabilistic classifier based on Bayes' Theorem. It relies on the \"naive\" assumption of conditional independence between every pair of features given the class label.\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "Based on Bayes' theorem, the probability of class $y$ given a set of features $X = (x_1, \\dots, x_n)$ is:\n",
    "$$P(y \\mid x_1, \\dots, x_n) = \\frac{P(x_1, \\dots, x_n \\mid y) P(y)}{P(x_1, \\dots, x_n)}$$\n",
    "\n",
    "Assuming independent features, this simplifies to maximizing the numerator:\n",
    "$$\\hat{y} = \\arg\\max_{y} P(y) \\prod_{i=1}^{n} P(x_i \\mid y)$$\n",
    "\n",
    "**Example Problem:**\n",
    "* **Natural Language Processing (NLP):** Classifying emails as \"Spam\" or \"Inbox\" based on the frequency of certain words (e.g., \"Free\", \"Winner\", \"Meeting\").\n",
    "\n",
    "The slider below controls `var_smoothing`, which artificially adds a tiny portion of the largest variance to all features to prevent division-by-zero errors and mathematically stabilize the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be7bf0e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a56a9c5f7f43e18a416fb16717fb9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatLogSlider(value=1e-09, description='Smoothing', max=-1.0, min=-10.0, step=1.0), Out…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(var_smoothing=FloatLogSlider(value=1e-9, min=-10, max=-1, step=1, description='Smoothing'))\n",
    "def plot_naive_bayes(var_smoothing):\n",
    "    nb_clf = GaussianNB(var_smoothing=var_smoothing)\n",
    "    nb_clf.fit(X_class, y_class)\n",
    "    plot_decision_boundary(nb_clf, X_class, y_class, f\"Gaussian Naive Bayes (Smoothing={var_smoothing:.1e})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787c1580",
   "metadata": {},
   "source": [
    "### 5. Decision Tree Classification\n",
    "\n",
    "A Decision Tree classifies data by recursively splitting the dataset based on feature values. It creates a flowchart-like tree structure by asking a sequence of true/false questions.\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "The tree splits nodes by finding the feature and threshold that minimize **Impurity**. A common metric is the **Gini Impurity**:\n",
    "$$Gini(Q) = 1 - \\sum_{k=1}^{C} p_{k}^2$$\n",
    "Where $p_k$ is the ratio of class $k$ instances among the training instances in node $Q$. The tree tries to make each resulting leaf node as homogeneous (pure) as possible.\n",
    "\n",
    "**Example Problem:**\n",
    "* **Medical Diagnosis:** Diagnosing a disease based on a checklist of symptoms (e.g., Does patient have fever? Yes $\\rightarrow$ Cough? Yes $\\rightarrow$ Flu).\n",
    "\n",
    "The slider below controls `max_depth`. A shallow tree may underfit the data, but an unrestricted deep tree will memorize the training data (overfitting) by creating microscopic decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "899c92d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fc2365a375d44789f5891140cdde87b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=3, description='Max Depth', max=15, min=1), Output()), _dom_classes=('wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(max_depth=IntSlider(min=1, max=15, step=1, value=3, description='Max Depth'))\n",
    "def plot_decision_tree(max_depth):\n",
    "    tree_clf = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
    "    tree_clf.fit(X_class, y_class)\n",
    "    plot_decision_boundary(tree_clf, X_class, y_class, f\"Decision Tree (Max Depth={max_depth})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
