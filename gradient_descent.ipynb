{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "943f1c13",
   "metadata": {},
   "source": [
    "# Core Concepts: How ML Learns the \"Shape\" of Data\n",
    "\n",
    "At its core, Machine Learning is about finding the underlying mathematical \"shape\" or pattern hidden inside a scatterplot of data points. To do this, almost all algorithms rely on a cycle of guessing, measuring the error, and adjusting. \n",
    "\n",
    "Here is how the fundamental pieces fit together.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. The Ruler: Loss Functions & MSE\n",
    "A **Loss Function** (or Cost Function) is the mathematical ruler we use to measure how \"wrong\" the model's current guess is. The model's entire goal in life is to minimize this number.\n",
    "\n",
    "For regression problems (predicting continuous numbers), the most common ruler is **Mean Squared Error (MSE)**. \n",
    "\n",
    "\n",
    "\n",
    "When you draw a line through data, you measure the vertical distance between each actual data point ($y_i$) and the point your line predicted $h_\\theta(x_i)$. These distances are called **residuals** or errors. MSE squares these distances (to remove negative signs and punish large errors heavily) and averages them:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( h_\\theta(x_i) - y_i \\right)^2$$\n",
    "\n",
    "*Note: In calculus-based machine learning proofs, you will often see MSE written multiplied by $\\frac{1}{2}$ (i.e., $\\frac{1}{2n}$). This is a mathematical convenience so that when you take the derivative, the $2$ from the exponent cancels out, leaving a cleaner formula.*\n",
    "\n",
    "**Why square the error?**\n",
    "1. It ensures all errors are positive (an error of $-5$ and $+5$ are equally bad).\n",
    "2. It heavily penalizes large outliers (an error of $10$ costs $100$).\n",
    "3. It creates a convex function (a bowl shape), ensuring there is only one global minimum to find.\n",
    "\n",
    "* **If the shape is wrong:** The line is far from the points, residuals are huge, and MSE is high.\n",
    "* **If the shape is right:** The line passes through or near the points, residuals are tiny, and MSE approaches zero.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. The Engine: Optimization (Gradient Descent)\n",
    "Knowing you are wrong is only half the battle; you have to know *how to fix it*. This is where optimization algorithms like **Gradient Descent** come in. \n",
    "\n",
    "Imagine the Loss Function as a multi-dimensional bowl. The algorithm computes the Gradient ($\\nabla J(\\theta)$), which is a vector containing the partial derivatives of the loss function with respect to every single parameter $\\theta_j$.The partial derivative tells us the slope of the error curve for a specific weight:$$\\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\frac{2}{n} \\sum_{i=1}^{n} \\left( h_\\theta(x_i) - y_i \\right) x_{ij}$$Because the gradient always points in the direction of the steepest ascent (uphill), we subtract it to move downhill toward the minimum error.\n",
    "\n",
    "Gradient Descent calculates the \"slope\" (gradient) of the hill at its current position and takes a step downward. As it adjusts its internal parameters (like the slope and intercept of a line), the line physically shifts on the graph, inching closer and closer to the true shape of the data points until it \"settles\" in the valley.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. The Learning Rate ($\\alpha$)\n",
    "The **Learning Rate** determines the *size of the step* the model takes while trying to find the minimum error with negative gradient direction.\n",
    "\n",
    "The mathematical update rule applied at every epoch is:$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}$$\n",
    "\n",
    "Where $\\frac{\\partial J}{\\partial \\theta}$ is the gradient (slope). If $\\alpha$ is huge, the subtraction term becomes huge, throwing the weights ($\\theta$) far away from the optimal values.\n",
    "\n",
    "* **Too Small ($\\alpha < 0.001$):** The model learns remarkably slowly. It might take 10,000 epochs to reach the solution that should have taken 100. The steps are microscopic. Convergence takes an impractical amount of time\n",
    "* **Just Right:** The model steadily descends the error mountain and settles in the valley.\n",
    "* **Too Large ($\\alpha > 1.0$):** The model takes massive steps, overshooting the valley entirely. It bounces back and forth, often getting worse (diverging) rather than better. Mathematically, if $\\alpha > \\frac{2}{L}$ (where $L$ is the Lipschitz constant of the gradient), the sequence will diverge, causing the error to explode toward infinity.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. The Shape-Shifter: Kernels\n",
    "Linear models and Gradient Descent are great if your data points form a straight line or a flat plane. But real-world data is messy—it forms curves, spirals, and clusters. \n",
    "\n",
    "If you try to fit a straight line to a U-shaped parabola, your MSE will always be terrible because a line simply cannot bend to match that shape.\n",
    "\n",
    "This is where the **Kernel Trick** steps in.\n",
    "\n",
    "\n",
    "\n",
    "Instead of forcing a rigid straight line to bend, a Kernel mathematically warps the *space itself*. It projects the 2D data points into a 3D (or higher) dimension. In this new, higher dimension, the data gets stretched out in such a way that a completely flat, straight plane can slide right through it. \n",
    "\n",
    "When you map that flat plane back down to your normal 2D screen, it looks like a perfectly drawn curve that hugs the complex shape of your data.\n",
    "\n",
    "Kernels map our input features $x$ into a higher-dimensional space using a mapping function $\\phi(x)$, where the data becomes linearly separable.However, computing $\\phi(x)$ for millions of points in infinite dimensions is computationally impossible. The Kernel Trick resolves this by replacing the dot product of the mapped features with a Kernel function $K$. We compute the similarity in the high-dimensional space without ever actually visiting it:$$K(x_i, x_j) = \\phi(x_i)^T \\phi(x_j)$$\n",
    "\n",
    "Common Mathematical Kernels:\n",
    "* **Polynomial Kernel:** $K(x, y) = (x^T y + c)^d$\n",
    "* **Radial Basis Function (RBF/Gaussian):** $K(x, y) = \\exp\\left(-\\frac{||x - y||^2}{2\\sigma^2}\\right)$(The RBF kernel implicitly maps data into an infinite-dimensional space!)\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Regularization: Ridge & Lasso\n",
    "Highly flexible models (like high-degree polynomials) are prone to overfitting—they memorize the training noise instead of the signal. Regularization adds a penalty term to the Loss Function to constrain the size of the parameters $\\theta$. \n",
    "\n",
    "**Regularization** forces the model to be \"simpler\" by adding a penalty to the loss function. It punishes the model for having large coefficients ($\\beta$).\n",
    "\n",
    "\n",
    "\n",
    "**A. Ridge Regression (L2 Regularization)**\n",
    "\n",
    "Ridge adds the *squared* magnitude of coefficients to the penalty. It shrinks coefficients toward zero but rarely *exactly* to zero.\n",
    "$$J(\\theta) = MSE(\\theta) + \\alpha \\sum_{j=1}^{p} \\theta_j^2$$\n",
    "\n",
    "*Mathematical flex*: Unlike standard Gradient Descent, Ridge Regression actually has a closed-form algebraic solution (Normal Equation):$$\\hat{\\theta} = (X^T X + \\lambda I)^{-1} X^T y$$\n",
    "*(The addition of the Identity matrix $I$ ensures the matrix is always invertible, solving collinearity issues).*\n",
    "\n",
    "**B. Lasso Regression (L1 Regularization)**\n",
    "\n",
    "Lasso adds the *absolute* value of coefficients to the penalty. It can shrink coefficients all the way to zero, effectively performing **Feature Selection** (removing useless features).\n",
    "$$J(\\theta) = MSE(\\theta) + \\alpha \\sum_{i=1}^{n} |\\theta_j|$$\n",
    "\n",
    "*Note: In Scikit-Learn, the regularization strength is called `alpha` (not to be confused with the Learning Rate).*\n",
    "\n",
    "---\n",
    "### 6. Evaluation Metrics\n",
    "How do we know if our model is actually \"good\"?\n",
    "\n",
    "1.  **Mean Absolute Error (MAE):** The average absolute difference between predicted and actual values. Robust to outliers.\n",
    "    $$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$$\n",
    "2.  **Root Mean Squared Error (RMSE):** The square root of MSE. It punishes large errors more than MAE and is in the same units as the target variable (e.g., dollars, degrees).\n",
    "    $$\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$\n",
    "3.  **R-Squared ($R^2$):** Represents the proportion of variance in the dependent variable that is predictable from the independent variable.\n",
    "   $$R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}$$\n",
    "   * $R^2 = 1$: Perfect fit.\n",
    "   * $R^2 = 0$: The model is no better than just guessing the mean value of $y$.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary: The ML Learning Loop\n",
    "1. **Initialize:** Define the hypothesis $h_\\theta(x)$ with random parameter weights $\\theta$.\n",
    "2. **Forward Pass:** Calculate the predictions $\\hat{y}$.\n",
    "3. **Measure (Loss Function):** Use $J(\\theta)$ to quantify the total error of the predictions.\n",
    "4. **Regularize:** Add the L1 ($\\lambda|\\theta|$) or L2 ($\\lambda\\theta^2$) penalty to the loss to prevent overfitting.\n",
    "5. **Calculate Gradients:** Compute $\\frac{\\partial J}{\\partial \\theta}$ to find the slope of the error landscape.\n",
    "6. **Update (Optimization):** Step downhill by updating weights: $\\theta := \\theta - \\alpha \\nabla J$.\n",
    "7. **Repeat:** Loop steps 2–6 until the gradient approaches zero (convergence).\n",
    "8. **Evaluate:** Score the final model using independent metrics like $R^2$ and RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ac4b2f",
   "metadata": {},
   "source": [
    "## Gradient Descent, Regularization & Metrics Visualization\n",
    "\n",
    "In this section, we will watch a **Polynomial Regression** model learn the shape of a curved dataset from scratch. We are manually applying **Gradient Descent** with **L2 Regularization (Ridge)**. \n",
    "\n",
    "The model starts with random weights. At each step (epoch), it calculates the Loss, evaluates its performance, finds the gradient, and updates its weights. You can interactively control the **Learning Rate** and **Regularization Strength** to see how they impact the training process!\n",
    "\n",
    "\n",
    "\n",
    "### The Math Behind the Animation\n",
    "1. **Hypothesis:** $\\hat{y} = \\theta_0 + \\theta_1 x + \\theta_2 x^2$\n",
    "2. **Loss Function (MSE + L2 Penalty):** $$J(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2 + \\lambda \\sum_{j=1}^{2} \\theta_j^2$$\n",
    "   *(Note: We do not penalize the bias term $\\theta_0$)*\n",
    "3. **Gradient Descent Update:** $$\\theta := \\theta - \\alpha \\nabla J(\\theta)$$\n",
    "   *(Where $\\alpha$ is the Learning Rate and $\\lambda$ is the Regularization penalty)*\n",
    "\n",
    "### Evaluation Metrics\n",
    "To know how well our model is doing, we track two key metrics at every epoch:\n",
    "* **RMSE (Root Mean Squared Error):** The average distance between the curve and the data points. Lower is better.\n",
    "* **$R^2$ (R-Squared):** How well the curve explains the variance of the data. 1.0 is perfect, 0.0 is basically a flat line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92c46d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98694d727cc14d069e6253afb7481e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Epoch', max=149), FloatLogSlider(value=0.1, description=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, IntSlider, FloatLogSlider\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Global plot settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# 1. Generate Non-Linear Data (A Parabola with noise)\n",
    "np.random.seed(42)\n",
    "m = 100\n",
    "X = 2 * np.random.rand(m, 1) - 1  # Range: -1 to 1\n",
    "y = 3 * X**2 + 1.5 * X + 2 + np.random.randn(m, 1) * 0.4 # True equation\n",
    "\n",
    "# 2. Prepare Polynomial Features (x^0, x^1, x^2)\n",
    "X_b = np.c_[np.ones((m, 1)), X, X**2]\n",
    "\n",
    "# 3. Dynamic Gradient Descent Function\n",
    "def run_gradient_descent(lr, lambda_reg, n_epochs=150):\n",
    "    # Initialize random weights (theta_0, theta_1, theta_2)\n",
    "    np.random.seed(10)\n",
    "    theta = np.random.randn(3, 1) \n",
    "    \n",
    "    theta_history = []\n",
    "    loss_history = []\n",
    "    metrics_history = [] # Will store (RMSE, R2)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Calculate predictions\n",
    "        y_predict = X_b.dot(theta)\n",
    "        \n",
    "        # Calculate Loss (MSE + L2 Regularization Penalty)\n",
    "        # We don't regularize the bias term (theta[0])\n",
    "        reg_penalty = lambda_reg * np.sum(theta[1:]**2)\n",
    "        loss = np.mean((y_predict - y)**2) + reg_penalty\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        # Calculate pure Evaluation Metrics (No penalty applied to the metric itself)\n",
    "        rmse = np.sqrt(np.mean((y_predict - y)**2))\n",
    "        r2 = 1 - (np.sum((y - y_predict)**2) / np.sum((y - np.mean(y))**2))\n",
    "        metrics_history.append((rmse, r2))\n",
    "        \n",
    "        # Save current weights\n",
    "        theta_history.append(theta.copy())\n",
    "        \n",
    "        # Calculate Gradients with Regularization\n",
    "        reg_grad = np.copy(theta)\n",
    "        reg_grad[0] = 0 # Don't penalize bias in the gradient\n",
    "        gradients = (2/m) * X_b.T.dot(y_predict - y) + 2 * lambda_reg * reg_grad\n",
    "        \n",
    "        # Update Weights\n",
    "        theta = theta - lr * gradients\n",
    "        \n",
    "    return theta_history, loss_history, metrics_history\n",
    "\n",
    "# 4. Interactive Visualization\n",
    "@interact(\n",
    "    epoch=IntSlider(min=0, max=149, step=1, value=0, description='Epoch'),\n",
    "    lr=FloatLogSlider(value=0.1, base=10, min=-3, max=-0.5, step=0.1, description='Learn Rate (α)'),\n",
    "    lambda_reg=FloatLogSlider(value=0.0001, base=10, min=-5, max=1, step=0.5, description='L2 Penalty (λ)')\n",
    ")\n",
    "def plot_learning_curve(epoch, lr, lambda_reg):\n",
    "    # Run the GD algorithm based on current slider parameters\n",
    "    theta_hist, loss_hist, metrics_hist = run_gradient_descent(lr, lambda_reg, 150)\n",
    "    \n",
    "    current_theta = theta_hist[epoch]\n",
    "    current_loss = loss_hist[epoch]\n",
    "    current_rmse, current_r2 = metrics_hist[epoch]\n",
    "    \n",
    "    # Generate smooth curve for plotting\n",
    "    X_new = np.linspace(-1.1, 1.1, 100).reshape(100, 1)\n",
    "    X_new_b = np.c_[np.ones((100, 1)), X_new, X_new**2]\n",
    "    y_predict_smooth = X_new_b.dot(current_theta)\n",
    "    \n",
    "    # Set up side-by-side plots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # --- Left Plot: The Data and the Curve ---\n",
    "    ax1.scatter(X, y, color='blue', alpha=0.5, label=\"Training Data\")\n",
    "    ax1.plot(X_new, y_predict_smooth, 'r-', linewidth=3, label=f\"Model Guess\")\n",
    "    ax1.set_title(\"Learning the Data's Shape\")\n",
    "    ax1.set_xlabel(\"X\")\n",
    "    ax1.set_ylabel(\"y\")\n",
    "    ax1.set_ylim(-1, 7)\n",
    "    \n",
    "    # Display the current equation and Metrics\n",
    "    metrics_text = (\n",
    "        f\"Eq: y = {current_theta[2][0]:.2f}x² + {current_theta[1][0]:.2f}x + {current_theta[0][0]:.2f}\\n\"\n",
    "        f\"RMSE: {current_rmse:.3f}\\n\"\n",
    "        f\"R²: {current_r2:.3f}\"\n",
    "    )\n",
    "    ax1.text(-1, 5.5, metrics_text, fontsize=12, bbox=dict(facecolor='white', alpha=0.9, edgecolor='black'))\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "    \n",
    "    # --- Right Plot: The Loss Curve (MSE + L2) ---\n",
    "    ax2.plot(range(epoch + 1), loss_hist[:epoch + 1], 'b-', linewidth=2)\n",
    "    ax2.scatter(epoch, current_loss, color='red', s=50, zorder=5) # Red dot at current epoch\n",
    "    ax2.set_title(f\"Gradient Descent Loss Curve (Current Loss: {current_loss:.4f})\")\n",
    "    ax2.set_xlabel(\"Epoch (Time)\")\n",
    "    ax2.set_ylabel(\"Cost Function J(θ)\")\n",
    "    ax2.set_xlim(0, 150)\n",
    "    \n",
    "    # Prevent plot scaling from blowing up if learning rate is too high\n",
    "    max_loss_display = min(loss_hist[0] * 2, max(loss_hist) + 1)\n",
    "    ax2.set_ylim(0, max_loss_display)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fead4956",
   "metadata": {},
   "source": [
    "Checking for **Overfitting** and **Underfitting** is the most critical part of the Machine Learning workflow. The only way to truly detect them is by comparing the model's performance on the **Training Data** (what it has seen) against **Validation/Test Data** (what it hasn't seen). \n",
    "\n",
    "### The Overfitting vs. Underfitting Framework. \n",
    "\n",
    "1. **Underfitting (High Bias)**\n",
    "   \n",
    "   The model is too simple to capture the underlying pattern.\n",
    "   * **Symptoms:** Both Training Loss and Validation Loss are high.\n",
    "   * **Visual Signal:** The regression line is \"too straight\" or doesn't follow the curve of the data.\n",
    "   * **Metric Signal:** Low $R^2$ on both training and test sets. \n",
    "\n",
    "2. **Overfitting (High Variance)**\n",
    "   \n",
    "   The model is too complex and has \"memorized\" the noise and outliers of the training set.\n",
    "   * **Symptoms:** Training Loss is very low, but Validation Loss is much higher.\n",
    "   * **Visual Signal:** The regression line is \"wiggly\" and tries to touch every single dot.\n",
    "   * **Metric Signal:** High $R^2$ on training data, but low/negative $R^2$ on validation data.\n",
    "\n",
    "3. **The Sweet Spot (Generalization)**\n",
    "   \n",
    "   The \"Goldilocks\" zone where the model captures the trend but ignores the noise.\n",
    "   * **Symptoms:** Training and Validation losses are both low and close to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b94e6e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec20cd3cb8d541d281df4095e6636fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=199, description='epoch', max=199), FloatLogSlider(value=0.1, descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, IntSlider, FloatLogSlider\n",
    "\n",
    "# 1. Generate Data and Split into Train/Validation\n",
    "np.random.seed(42)\n",
    "m = 100\n",
    "X = 2 * np.random.rand(m, 1) - 1\n",
    "y = 3 * X**2 + 1.5 * X + 2 + np.random.randn(m, 1) * 0.4\n",
    "\n",
    "# Split 80/20\n",
    "train_idx = np.random.choice(m, 80, replace=False)\n",
    "val_idx = np.array([i for i in range(m) if i not in train_idx])\n",
    "\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_val, y_val = X[val_idx], y[val_idx]\n",
    "\n",
    "# Prepare Polynomial Features\n",
    "X_train_b = np.c_[np.ones((80, 1)), X_train, X_train**2]\n",
    "X_val_b = np.c_[np.ones((20, 1)), X_val, X_val**2]\n",
    "\n",
    "def run_split_gd(lr, lambda_reg, n_epochs=200):\n",
    "    np.random.seed(10)\n",
    "    theta = np.random.randn(3, 1)\n",
    "    train_loss_hist, val_loss_hist = [], []\n",
    "    theta_hist = []\n",
    "\n",
    "    for _ in range(n_epochs):\n",
    "        # Training Step\n",
    "        y_train_pred = X_train_b.dot(theta)\n",
    "        train_loss = np.mean((y_train_pred - y_train)**2) + lambda_reg * np.sum(theta[1:]**2)\n",
    "        train_loss_hist.append(train_loss)\n",
    "        \n",
    "        # Validation Step (No gradients, no updates here!)\n",
    "        y_val_pred = X_val_b.dot(theta)\n",
    "        val_loss = np.mean((y_val_pred - y_val)**2) # Validation doesn't include penalty\n",
    "        val_loss_hist.append(val_loss)\n",
    "        \n",
    "        theta_hist.append(theta.copy())\n",
    "\n",
    "        # Update\n",
    "        reg_grad = np.copy(theta); reg_grad[0] = 0\n",
    "        gradients = (2/80) * X_train_b.T.dot(y_train_pred - y_train) + 2 * lambda_reg * reg_grad\n",
    "        theta = theta - lr * gradients\n",
    "        \n",
    "    return theta_hist, train_loss_hist, val_loss_hist\n",
    "\n",
    "@interact(\n",
    "    epoch=IntSlider(min=0, max=199, value=199, description=\"epoch\"),\n",
    "    lr=FloatLogSlider(value=0.1, min=-3, max=-0.5, description='Learn Rate (α)'),\n",
    "    lambda_reg=FloatLogSlider(value=0.0001, min=-5, max=1, description='L2 Penalty (λ)')\n",
    ")\n",
    "def plot_fit_analysis(epoch, lr, lambda_reg):\n",
    "    t_hist, train_loss, val_loss = run_split_gd(lr, lambda_reg)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Left: Data Fit\n",
    "    X_new = np.linspace(-1.1, 1.1, 100).reshape(-1, 1)\n",
    "    X_new_b = np.c_[np.ones((100, 1)), X_new, X_new**2]\n",
    "    ax1.scatter(X_train, y_train, color='blue', alpha=0.5, label='Train Data')\n",
    "    ax1.scatter(X_val, y_val, color='orange', marker='s', label='Validation Data')\n",
    "    ax1.plot(X_new, X_new_b.dot(t_hist[epoch]), 'r-', lw=3)\n",
    "    ax1.set_title(\"Model Fit\")\n",
    "    ax1.legend()\n",
    "\n",
    "    # Right: Learning Curves (The Detection Tool)\n",
    "    ax2.plot(train_loss[:epoch+1], 'b-', label='Train Loss')\n",
    "    ax2.plot(val_loss[:epoch+1], 'orange', label='Val Loss')\n",
    "    ax2.set_title(f\"Train vs Val Loss (Gap: {abs(train_loss[epoch]-val_loss[epoch]):.4f})\")\n",
    "    ax2.set_xlabel(\"Epochs\")\n",
    "    ax2.set_ylabel(\"MSE\")\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Logic for detection\n",
    "    if val_loss[epoch] > train_loss[epoch] * 1.5:\n",
    "        ax2.text(epoch*0.1, max(train_loss)*0.8, \"WARNING: OVERFITTING\", color='red', fontsize=15, fontweight='bold')\n",
    "    elif train_loss[epoch] > 1.0:\n",
    "        ax2.text(epoch*0.1, max(train_loss)*0.8, \"WARNING: UNDERFITTING\", color='brown', fontsize=15, fontweight='bold')\n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
